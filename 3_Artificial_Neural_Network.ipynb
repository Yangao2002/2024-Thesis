{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b209d2-aa60-4d97-a027-b2269e60c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score,accuracy_score,make_scorer, confusion_matrix, classification_report,accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score,cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, LeakyReLU\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import ELU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import precision_recall_curve, fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64555c19-0a2c-4faa-9877-77f276c85425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data type for ANN\n",
    "\n",
    "cov_columns1 = imputed_result.select_dtypes(include=['bool']).columns # find bool\n",
    "cov_columns2 = imputed_result.select_dtypes(include=['int', 'int32', 'int64','float64']).columns # find int\n",
    "cov_columns = cov_columns1.union(cov_columns2)\n",
    "imputed_result[cov_columns] = imputed_result[cov_columns].astype('float32') # convert those types to float\n",
    "imputed_result['hypertension'] = imputed_result['hypertension'].astype('int32') # convert those types to float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00fc2d-1262-4625-8d5b-6e2af98a4edd",
   "metadata": {},
   "source": [
    "### **Split the data into train and test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5d2c0f-14b7-4026-8f52-b81cc3adff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "\n",
    "train_data, test_data = train_test_split(\n",
    "    imputed_result,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=imputed_result[['hypertension', 'age_group', 'male']]\n",
    ")\n",
    "\n",
    "test_male = test_data[test_data['male'] == 1]\n",
    "test_female = test_data[test_data['male'] == 0]\n",
    "test_age_1 = test_data[test_data['age_group'] == 1]\n",
    "test_age_2 = test_data[test_data['age_group'] == 2]\n",
    "test_age_3 = test_data[test_data['age_group'] == 3]\n",
    "test_age_4 = test_data[test_data['age_group'] == 4]\n",
    "test_age_5 = test_data[test_data['age_group'] == 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b2b02-35fb-493c-bd72-b334501ed3dc",
   "metadata": {},
   "source": [
    "### **Features sets for training various models** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e957e-9c41-47d5-b037-b8cbf1a608ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 common risk factors\n",
    "\n",
    "base_variables = [\n",
    "    'male', \n",
    "    'age_group', \n",
    "    'BMI', \n",
    "    'chest_pain', \n",
    "    'angina', \n",
    "    'heart_dis', \n",
    "    'stroke', \n",
    "    'h_cholesterol', \n",
    "    'diabetes', \n",
    "    'smk'\n",
    "]\n",
    "\n",
    "# Model 2 common risk factors + psychological features\n",
    "\n",
    "variables_2 = base_variables + [\n",
    "    'anxiety',\n",
    "    'sleeping_prb', \n",
    "    'happiness',\n",
    "    'life_satisfaction_x'\n",
    "]   \n",
    "\n",
    "# Model 3 common risk factors + psychological features + common SES features\n",
    "\n",
    "variables_3 = base_variables + [\n",
    "    'anxiety',\n",
    "    'sleeping_prb', \n",
    "    'happiness',\n",
    "    'life_satisfaction_x',\n",
    "    'marr_Married',\n",
    "    'marr_Widow_widower',\n",
    "    'marr_Never_been_married',\n",
    "    'marr_Divorced_separated',\n",
    "    'occu_Is pensioner',\n",
    "    'occu_No employment',\n",
    "    'occu_Paid employment',\n",
    "    'edu_cls'\n",
    "]   \n",
    "\n",
    "# Model 4 common risk factors + psychological features + common SES features + incomes + debts\n",
    "\n",
    "variables_4 = base_variables + [\n",
    "    'anxiety',\n",
    "    'sleeping_prb', \n",
    "    'happiness',\n",
    "    'life_satisfaction_x',\n",
    "    'marr_Married',\n",
    "    'marr_Widow_widower',\n",
    "    'marr_Never_been_married',\n",
    "    'marr_Divorced_separated',\n",
    "    'occu_Is pensioner',\n",
    "    'occu_No employment',\n",
    "    'occu_Paid employment',\n",
    "    'edu_cls',\n",
    "    'inc_cls3_Low',\n",
    "    'inc_cls3_Medium',\n",
    "    'inc_cls3_High',\n",
    "    'inc_cls3_Very_High',\n",
    "    'db_ttl2_cls_None',\n",
    "    'db_ttl2_cls_Low',\n",
    "    'db_ttl2_cls_Medium',\n",
    "    'db_ttl2_cls_High',\n",
    "    'db_ttl2_cls_Very_High'\n",
    "]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e926b8-94ad-453e-a016-3aace26e14a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables and random seed\n",
    "def seed_everything(seed=42, use_cpu=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_NUM_INTEROP_THREADS'] = '1'\n",
    "    os.environ['TF_NUM_INTRAOP_THREADS'] = '1'\n",
    "    if use_cpu:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# Set environment variables before importing TensorFlow\n",
    "seed_everything(42, use_cpu=True)\n",
    "\n",
    "# Set TensorFlow random seed and configure threading\n",
    "tf.random.set_seed(42)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4d0349-0748-43f3-a38b-5c9174a3be4f",
   "metadata": {},
   "source": [
    "### **Build Artificial Neural Network model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27731ad3-a288-4369-86b3-7b89e6a017e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setting\n",
    "def create_model(input_shape, \n",
    "                 learning_rate=0.01, \n",
    "                 l2_rate=0.01, \n",
    "                 neurons=32  \n",
    "                ):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        # Single hidden layer with L2 regularization\n",
    "        Dense(neurons, kernel_regularizer=l2(l2_rate)),\n",
    "        LeakyReLU(negative_slope=0.01),   # Leaky ReLU activation function\n",
    "        # Output layer: single neuron with Sigmoid activation for binary classification\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', 'auc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db9ea9-b9e4-4015-9ac8-c54f3957ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN with cross-validation and SMOTE\n",
    "def evaluate_ann(data, \n",
    "                 predictors, \n",
    "                 target, \n",
    "                 learning_rate=0.001, \n",
    "                 l2_rate=0.01, \n",
    "                 epochs=100, \n",
    "                 batch_size=10,\n",
    "                 cv_splits=5,\n",
    "                 threshold=0.5,\n",
    "                 k_neighbors=5,\n",
    "                 neurons=10):\n",
    "    X = data[predictors].values\n",
    "    y = data[target].values\n",
    "\n",
    "    # Initialize cross-validation\n",
    "    kf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Store metrics for each fold\n",
    "    auc_scores = []\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    f2_scores = []\n",
    "    recall_scores = []\n",
    "    precision_scores = []\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "    histories = []  # To store history of each fold\n",
    "\n",
    "    # Start cross-validation\n",
    "    for train_index, val_index in kf.split(X, y):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "    # Apply SMOTE to the training data\n",
    "        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Create model\n",
    "        model = create_model(input_shape=(X_train.shape[1],), \n",
    "                             learning_rate=learning_rate, \n",
    "                             l2_rate=l2_rate,\n",
    "                             neurons=neurons)\n",
    "\n",
    "        # Early stopping callback\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        # Train model with early stopping\n",
    "        history = model.fit(X_train_resampled, \n",
    "                            y_train_resampled, \n",
    "                            epochs=epochs, \n",
    "                            batch_size=batch_size, \n",
    "                            verbose=0, \n",
    "                            validation_data=(X_val, y_val),\n",
    "                            callbacks=[early_stopping])\n",
    "        \n",
    "        # Store the history\n",
    "        histories.append(history.history)\n",
    "        \n",
    "        # Perform predictions\n",
    "        pred_probs = model.predict(X_val).flatten()\n",
    "        predictions = (pred_probs > threshold).astype(int)\n",
    "\n",
    "        # Store true and predicted values for the entire dataset\n",
    "        y_true_all.extend(y_val)\n",
    "        y_pred_all.extend(predictions)\n",
    "\n",
    "        # Calculate metrics for this fold\n",
    "        auc_score = roc_auc_score(y_val, pred_probs)\n",
    "        accuracy = accuracy_score(y_val, predictions)\n",
    "        f1 = f1_score(y_val, predictions)\n",
    "        f2 = fbeta_score(y_val, predictions, beta=2)\n",
    "        recall = recall_score(y_val, predictions)\n",
    "        precision = precision_score(y_val, predictions)\n",
    "\n",
    "        # Store metrics\n",
    "        auc_scores.append(auc_score)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "        f2_scores.append(f2)\n",
    "        recall_scores.append(recall)\n",
    "        precision_scores.append(precision)\n",
    "\n",
    "    # Calculate confusion matrix and classification report for the entire dataset\n",
    "    conf_matrix = confusion_matrix(y_true_all, y_pred_all)\n",
    "    report = classification_report(y_true_all, y_pred_all)\n",
    "\n",
    "    # Calculate mean and standard deviation for other metrics\n",
    "    results = {\n",
    "        'Average AUC': np.mean(auc_scores),\n",
    "        'AUC SD': np.std(auc_scores),\n",
    "        'Average Accuracy': np.mean(accuracy_scores),\n",
    "        'Accuracy SD': np.std(accuracy_scores),\n",
    "        'Average F1 Score': np.mean(f1_scores),\n",
    "        'F1 Score SD': np.std(f1_scores),\n",
    "        'Average F2 Score': np.mean(f2_scores),\n",
    "        'F2 Score SD': np.std(f2_scores),\n",
    "        'Average Recall': np.mean(recall_scores),\n",
    "        'Recall SD': np.std(recall_scores),\n",
    "        'Average Precision': np.mean(precision_scores),\n",
    "        'Precision SD': np.std(precision_scores),\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Classification Report': report,\n",
    "        'Histories': histories,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "    return results\n",
    "    \n",
    "def print_evaluation_results(results):\n",
    "    print(\"Evaluation Results:\")\n",
    "    print(f\"Average AUC: {results['Average AUC']:.4f} ± {results['AUC SD']:.4f}\")\n",
    "    print(f\"Average Accuracy: {results['Average Accuracy']:.4f} ± {results['Accuracy SD']:.4f}\")\n",
    "    print(f\"Average F1 Score: {results['Average F1 Score']:.4f} ± {results['F1 Score SD']:.4f}\")\n",
    "    print(f\"Average F2 Score: {results['Average F2 Score']:.4f} ± {results['F2 Score SD']:.4f}\")\n",
    "    print(f\"Average Recall: {results['Average Recall']:.4f} ± {results['Recall SD']:.4f}\")\n",
    "    print(f\"Average Precision: {results['Average Precision']:.4f} ± {results['Precision SD']:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(results['Confusion Matrix'])\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(results['Classification Report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39723ece-04e8-48f5-ac03-d1d83bfc1c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate random forest model on test set\n",
    "\n",
    "def evaluate_on_test(final_model, test_data, predictors, target, threshold=0.5):\n",
    "    X_test = test_data[predictors].values\n",
    "    y_test = test_data[target].values\n",
    "\n",
    "    test_pred_probs = final_model.predict(X_test).flatten()\n",
    "    test_predictions = (test_pred_probs > threshold).astype(int)\n",
    "\n",
    "    test_auc_score = roc_auc_score(y_test, test_pred_probs)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    test_f1 = f1_score(y_test, test_predictions)\n",
    "    test_f2 = fbeta_score(y_test, test_predictions, beta=2)\n",
    "    test_recall = recall_score(y_test, test_predictions)\n",
    "    test_precision = precision_score(y_test, test_predictions)\n",
    "    test_conf_matrix = confusion_matrix(y_test, test_predictions)\n",
    "    test_report = classification_report(y_test, test_predictions)\n",
    "\n",
    "    print(\"Test Results:\")\n",
    "    print(f\"AUC: {test_auc_score:.4f}\")\n",
    "    print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {test_f1:.4f}\")\n",
    "    print(f\"F2 Score: {test_f2:.4f}\")\n",
    "    print(f\"Recall: {test_recall:.4f}\")\n",
    "    print(f\"Precision: {test_precision:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(test_conf_matrix)\n",
    "    print(\"Classification Report:\")\n",
    "    print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03087ea8-c012-4ebf-ac7a-c368fb58f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of loss and accuracy\n",
    "\n",
    "def plot_loss_and_accuracy(histories):\n",
    "    # Find the minimum number of epochs across all histories to align them\n",
    "    min_epochs = min(len(h['loss']) for h in histories if 'loss' in h)\n",
    "\n",
    "    # Truncate each history to have the same length\n",
    "    for h in histories:\n",
    "        for key in h.keys():\n",
    "            h[key] = h[key][:min_epochs]\n",
    "\n",
    "    # Compute average and standard deviation for each epoch across folds\n",
    "    avg_loss = np.mean([h['loss'] for h in histories if 'loss' in h], axis=0)\n",
    "    avg_val_loss = np.mean([h['val_loss'] for h in histories if 'val_loss' in h], axis=0)\n",
    "    std_loss = np.std([h['loss'] for h in histories if 'loss' in h], axis=0)\n",
    "    std_val_loss = np.std([h['val_loss'] for h in histories if 'val_loss' in h], axis=0)\n",
    "\n",
    "    avg_accuracy = np.mean([h['accuracy'] for h in histories if 'accuracy' in h], axis=0)\n",
    "    avg_val_accuracy = np.mean([h['val_accuracy'] for h in histories if 'val_accuracy' in h], axis=0)\n",
    "    std_accuracy = np.std([h['accuracy'] for h in histories if 'accuracy' in h], axis=0)\n",
    "    std_val_accuracy = np.std([h['val_accuracy'] for h in histories if 'val_accuracy' in h], axis=0)\n",
    "\n",
    "    # Check if 'auc' and 'val_auc' are available in histories\n",
    "    if 'auc' in histories[0] and 'val_auc' in histories[0]:\n",
    "        avg_auc = np.mean([h['auc'] for h in histories if 'auc' in h], axis=0)\n",
    "        avg_val_auc = np.mean([h['val_auc'] for h in histories if 'val_auc' in h], axis=0)\n",
    "        std_auc = np.std([h['auc'] for h in histories if 'auc' in h], axis=0)\n",
    "        std_val_auc = np.std([h['val_auc'] for h in histories if 'val_auc' in h], axis=0)\n",
    "\n",
    "    epochs = range(1, min_epochs + 1)\n",
    "\n",
    "    plt.figure(figsize=(18, 5))\n",
    "\n",
    "    # Plot loss curve with confidence interval\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, avg_loss, label='Train Loss', color='blue')\n",
    "    plt.fill_between(epochs, avg_loss - std_loss, avg_loss + std_loss, color='blue', alpha=0.2)\n",
    "    plt.plot(epochs, avg_val_loss, label='Validation Loss', color='orange')\n",
    "    plt.fill_between(epochs, avg_val_loss - std_val_loss, avg_val_loss + std_val_loss, color='orange', alpha=0.2)\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Plot accuracy curve with confidence interval\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, avg_accuracy, label='Train Accuracy', color='blue')\n",
    "    plt.fill_between(epochs, avg_accuracy - std_accuracy, avg_accuracy + std_accuracy, color='blue', alpha=0.2)\n",
    "    plt.plot(epochs, avg_val_accuracy, label='Validation Accuracy', color='orange')\n",
    "    plt.fill_between(epochs, avg_val_accuracy - std_val_accuracy, avg_val_accuracy + std_val_accuracy, color='orange', alpha=0.2)\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    # Plot AUC curve with confidence interval if available\n",
    "    if 'auc' in histories[0] and 'val_auc' in histories[0]:\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(epochs, avg_auc, label='Train AUC', color='blue')\n",
    "        plt.fill_between(epochs, avg_auc - std_auc, avg_auc + std_auc, color='blue', alpha=0.2)\n",
    "        plt.plot(epochs, avg_val_auc, label='Validation AUC', color='orange')\n",
    "        plt.fill_between(epochs, avg_val_auc - std_val_auc, avg_val_auc + std_val_auc, color='orange', alpha=0.2)\n",
    "        plt.title('Model AUC')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(loc='lower right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0853cf-2526-4380-9610-7bea38259fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the optimal threshold balancing precision and recall\n",
    "\n",
    "def plot_precision_recall_vs_threshold(model, data, predictors, target):\n",
    "    # Ensure your model outputs probabilities (not just class labels)\n",
    "    X_test = data[predictors]\n",
    "    y_test = data[target]\n",
    "    y_scores = model.predict(X_test).ravel()  # Flatten array if necessary\n",
    "\n",
    "    # Calculate precision, recall, and thresholds\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "    # Calculate F1 scores for each threshold\n",
    "    # f1_scores = np.where((precision + recall) == 0, 0, 2 * (precision * recall) / (precision + recall))\n",
    "    f1_scores = np.where((precision + recall) == 0, 0, 2*(precision*recall)/(precision+recall))\n",
    " \n",
    "    # Find the best threshold\n",
    "    best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    best_f1_score = np.max(f1_scores)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, precision[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recall[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.plot(thresholds, f1_scores[:-1], \"r-\", label=\"F1 Score\")\n",
    "    plt.title(\"Precision, Recall, and F1 Score for different thresholds\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.axvline(x=best_threshold, color='gray', linestyle='--', label=f'Best Threshold: {best_threshold:.4f}')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Best F1 Score: {best_f1_score:.4f} at Threshold: {best_threshold:.4f}\")\n",
    "\n",
    "# plot_precision_recall_vs_threshold(results['model'], test_data, variables_9, 'hypertension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e31880f-79b9-4e42-9c0c-6343282f8e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna for optimizing random forest parameters\n",
    "\n",
    "def objective(trial, data, predictors, target):\n",
    "    # Suggest hyperparameters to be optimized\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2])\n",
    "    l2_rate = trial.suggest_categorical('l2_rate', [1e-5, 1e-4, 1e-3, 1e-2,1e-1])\n",
    "    neurons = trial.suggest_int('neurons', 10, 30, step=2)\n",
    "    epochs = trial.suggest_categorical('epochs', [200])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [10, 15, 20])\n",
    "    k_neighbors = trial.suggest_int('k_neighbors', 3,9)\n",
    "    # Perform cross-validation with suggested hyperparameters\n",
    "    results = evaluate_ann(data=data, \n",
    "                           predictors=predictors, \n",
    "                           target=target, \n",
    "                           learning_rate=learning_rate, \n",
    "                           l2_rate=l2_rate, \n",
    "                           epochs=epochs, \n",
    "                           batch_size=batch_size, \n",
    "                           cv_splits=5,\n",
    "                           k_neighbors=k_neighbors,\n",
    "                           neurons=neurons)\n",
    "    return results['Average F1 Score']\n",
    "\n",
    "def optimize_hyperparameters(data, predictors, target, n_trials=50):\n",
    "    # Use partial to pass additional arguments to the objective function\n",
    "    objective_func = partial(objective, data=data, predictors=predictors, target=target)\n",
    "\n",
    "    # Create a study object and optimize the objective function\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    # study.optimize(objective_func, n_trials=n_trials)\n",
    "    study.optimize(objective_func, n_trials=n_trials, show_progress_bar=False, catch=(Exception,), callbacks=[], n_jobs=1) #without showing the process\n",
    "\n",
    "    # Output the best hyperparameters\n",
    "    print('Best Trial:')\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(f'  Best F1 Score: {trial.value}')\n",
    "    print('  Best hyperparameters:')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'    {key}: {value}')\n",
    "    \n",
    "    return trial.params\n",
    "\n",
    "# example\n",
    "# seed_everything(42, use_cpu=True)\n",
    "# best_params = optimize_hyperparameters(train_data, variables_4, 'hypertension', n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb1b96-b5aa-4618-926e-f661c004a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42, use_cpu=True)\n",
    "best_params = optimize_hyperparameters(train_data, base_variables, 'hypertension', n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f0c5c-343b-4a94-8c4e-30993b96a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9ab99-7d86-4fd9-ab4e-0c3de8a60de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 with Optuna-tuned parameters\n",
    "\n",
    "seed_everything(42, use_cpu=True)\n",
    "\n",
    "best_params = {'learning_rate': 0.001,\n",
    " 'l2_rate': 0.001,\n",
    " 'neurons': 24,\n",
    " 'epochs': 200,\n",
    " 'batch_size': 10,\n",
    " 'k_neighbors': 5}\n",
    "\n",
    "results = evaluate_ann(\n",
    "    data=train_data, \n",
    "    predictors=base_variables, \n",
    "    target='hypertension', \n",
    "    learning_rate=best_params['learning_rate'], \n",
    "    l2_rate=best_params['l2_rate'], \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'],\n",
    "    neurons=best_params['neurons'],\n",
    "    k_neighbors=best_params['k_neighbors']\n",
    ")\n",
    "\n",
    "print_evaluation_results(results)\n",
    "plot_loss_and_accuracy(results['Histories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224f3213-b69d-4532-8f17-d9bf0faa075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_test(results['model'], test_data,base_variables, 'hypertension',threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e366ec5-9ebd-42ac-b022-b1149f27c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42, use_cpu=True)\n",
    "best_params2 = optimize_hyperparameters(train_data, variables_2, 'hypertension', n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf53f6-bd6d-4ea3-a4ae-601fe8b459a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff15141-7953-4473-af0b-0167db36447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 with Optuna-tuned parameters\n",
    "\n",
    "seed_everything(42, use_cpu=True)\n",
    "\n",
    "best_params = {'learning_rate': 0.001,\n",
    " 'l2_rate': 0.01,\n",
    " 'neurons': 26,\n",
    " 'epochs': 200,\n",
    " 'batch_size': 15,\n",
    " 'k_neighbors': 5}\n",
    "\n",
    "results = evaluate_ann(\n",
    "    data=train_data, \n",
    "    predictors=variables_2, \n",
    "    target='hypertension', \n",
    "    learning_rate=best_params['learning_rate'], \n",
    "    l2_rate=best_params['l2_rate'], \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'],\n",
    "    neurons=best_params['neurons'],\n",
    "    k_neighbors=best_params['k_neighbors']\n",
    ")\n",
    "\n",
    "print_evaluation_results(results)\n",
    "plot_loss_and_accuracy(results['Histories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e554c2b6-2bbb-4078-bdc6-1a0872ec8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_test(results['model'], test_data,variables_2, 'hypertension',threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055e8a87-4456-46ee-81e7-00243d378408",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42, use_cpu=True)\n",
    "best_params3 = optimize_hyperparameters(train_data, variables_3, 'hypertension', n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc08fcd-bd2a-4eed-a1f8-0e15923fb244",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb64f0-1fbb-408b-977c-845d04c5dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 with Optuna-tuned parameters\n",
    "\n",
    "seed_everything(42, use_cpu=True)\n",
    "\n",
    "best_params = {'learning_rate': 0.0001,\n",
    " 'l2_rate': 0.001,\n",
    " 'neurons': 28,\n",
    " 'epochs': 200,\n",
    " 'batch_size': 20,\n",
    " 'k_neighbors': 3}\n",
    "\n",
    "results = evaluate_ann(\n",
    "    data=train_data, \n",
    "    predictors=variables_3, \n",
    "    target='hypertension', \n",
    "    learning_rate=best_params['learning_rate'], \n",
    "    l2_rate=best_params['l2_rate'], \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'],\n",
    "    neurons=best_params['neurons'],\n",
    "    k_neighbors=best_params['k_neighbors']\n",
    ")\n",
    "\n",
    "print_evaluation_results(results)\n",
    "plot_loss_and_accuracy(results['Histories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef54f7-10d5-4769-a6a7-cc0d6ffb00f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_test(results['model'], test_data,variables_3, 'hypertension',threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4656f-b62c-4f97-90cd-d952308241cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42, use_cpu=True)\n",
    "best_params4 = optimize_hyperparameters(train_data, variables_4, 'hypertension', n_trials=100)\n",
    "# 0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b119d2d-c1c0-44b4-a4ae-6c9c2a6e490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4859d07-56af-44d8-95e6-2bc41b59d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4 with Optuna-tuned parameters\n",
    "\n",
    "seed_everything(42, use_cpu=True)\n",
    "\n",
    "best_params = {'learning_rate': 0.0001,\n",
    " 'l2_rate': 0.0001,\n",
    " 'neurons': 18,\n",
    " 'epochs': 200,\n",
    " 'batch_size': 10,\n",
    " 'k_neighbors': 6}\n",
    "\n",
    "results = evaluate_ann(\n",
    "    data=train_data, \n",
    "    predictors=variables_4, \n",
    "    target='hypertension', \n",
    "    learning_rate=best_params['learning_rate'], \n",
    "    l2_rate=best_params['l2_rate'], \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'],\n",
    "    neurons=best_params['neurons'],\n",
    "    k_neighbors=best_params['k_neighbors']\n",
    ")\n",
    "\n",
    "print_evaluation_results(results)\n",
    "plot_loss_and_accuracy(results['Histories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f88815-2ea3-4a9e-8a4d-38f1d31b561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_test(results['model'], test_data,variables_4, 'hypertension',threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32ebba-4291-4431-811c-e7aa1190f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Male\n",
    "evaluate_on_test(results['model'], test_male,variables_4, 'hypertension',threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5fb341-1b44-4e19-8aa6-85fce725600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Female\n",
    "evaluate_on_test(results['model'], test_female,variables_4, 'hypertension',threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee9c794-6dea-40ae-9212-f15ed6785a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_test(results['model'], test_age_1,variables_4, 'hypertension',threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d8cbf5-97f2-4891-a488-33693939b968",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_test(results['model'], test_age_2,variables_4, 'hypertension',threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90145985-a90e-4c73-adbc-cb8ad885604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_test(results['model'], test_age_3,variables_4, 'hypertension',threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0fe41-51a6-4391-9535-fd77f97ae268",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_test(results['model'], test_age_4,variables_4, 'hypertension',threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75467c2-4ab8-4e99-b4f8-08237c38c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_test(results['model'], test_age_5,variables_4, 'hypertension',threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bcb978-b489-4d08-b7b9-cf3814d3d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_vs_threshold(results['model'], test_data, variables_4, 'hypertension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a0779-d185-4f09-85f5-1de3d6bfb441",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_test(results['model'], test_data,variables_4, 'hypertension',threshold = 0.4291)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
